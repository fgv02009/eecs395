{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Newtonâ€™s method to compute a square root of the number: 999. Briefly explain how you set up the relevant cost function that was minimized to obtain this square root. Explain how you use zero or first order optimization methods (detailed in Chapters 2 and 3) to do this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True   \n",
    "import matplotlib.pyplot as plt\n",
    "# import statment for gradient calculator\n",
    "from autograd import grad\n",
    "from autograd import numpy as np\n",
    "from autograd import value_and_grad\n",
    "from autograd import hessian\n",
    "\n",
    "def newtons_method(g, max_its, w, **kwargs):\n",
    "    gradient = grad(g)\n",
    "    #eigenvals of a square matrix -- C in our example\n",
    "    hess = hessian(g)\n",
    "    \n",
    "    epsilon = 10**(-7)\n",
    "    if 'epsilon' in kwargs:\n",
    "        epsilon = kwargs['epsilon']\n",
    "        \n",
    "    weight_history = [w]\n",
    "    cost_history = [g(w)]\n",
    "    \n",
    "    for k in range(max_its):\n",
    "        grad_eval = gradient(w)\n",
    "        hess_eval = hess(w)\n",
    "        \n",
    "        hess_eval.shape = (int((np.size(hess_eval))**(.5)), int((np.size(hess_eval))**(.5)))\n",
    "        #we're assuming it's positive aka convex\n",
    "        \n",
    "        print(\"A will be: \", hess_eval + epsilon*np.eye(w.size))\n",
    "        #look at 4.8\n",
    "        A = hess_eval + epsilon*np.eye(w.size)\n",
    "        b = grad_eval\n",
    "        \n",
    "        #I don't follow how the below gets us to our next point\n",
    "        w = np.linalg.solve(A,np.dot(A,w) - b)\n",
    "        \n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "    return weight_history, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_point = 2.5\n",
    "init_point = np.array([2.5])\n",
    "g = lambda w: (1/50)*(w**4 + w**2 + 10*w) + .5\n",
    "#based on 3.6 of previous hw i feel like this should minimize at -.2\n",
    "iterations = 10\n",
    "\n",
    "a1_wh, a1_ch = newtons_method(g, iterations, init_point)\n",
    "# a2_wh, a2_ch = norm_gradient_descent(g, iterations, init_point, 'full')\n",
    "# a3_wh, a3_ch = norm_gradient_descent(g, iterations, init_point, 'component')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(a1_ch, 'k-', label = 'Newtons Method')\n",
    "# plt.plot(a2_ch, 'm-', label = 'Fully Normalized grad descent')\n",
    "# plt.plot(a3_ch, 'b-', label = 'Component Normalized grad descent')\n",
    "plt.title(\"Cost Function History Plot\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"step k\")\n",
    "plt.ylabel(\"g(w^k)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
